#!/usr/bin/env bash
# Required environment variables:
# batch_size (recommendation: 8 / 16)
# lr: learning rate (recommendation: 3e-5 / 5e-5)
# seed: random seed, default is 1234
# BERT_NAME: pre-trained text model name ( bert-*)
# max_seq: max sequence length
# sample_ratio: few-shot learning, default is 1.0
# save_path: model saved path

DATASET_NAME="twitter15"
BERT_NAME="bert-base-uncased"

lr=3e-5

nohup CUDA_VISIBLE_DEVICES=7 python -u run.py
        --dataset_name=twitter15
        --bert_name=/home/nlp/code/HVPNeT/bert_base_uncased
        --num_epochs=30
        --batch_size=8
        --lr=3e-5
        --warmup_ratio=0.01
        --eval_begin_epoch=3
        --seed=1234
        --do_train
        --ignore_idx=0
        --max_seq=80
        --use_prompt
        --prompt_len=4
        --sample_ratio=1.0
        --save_path=your_ckpt_path  &
DATASET_NAME="twitter17"
BERT_NAME="bert-base-uncased"
twitter17num_epochs30batch_size32lr3e-5seed1234
CUDA_VISIBLE_DEVICES=0 python -u run.py \
        --dataset_name=twitter15
        --bert_name=/home/nlp/code/HVPNeT/bert_base_uncased
        --num_epochs=50
        --batch_size=32
        --lr=3e-5
        --warmup_ratio=0.01
        --eval_begin_epoch=3
        --seed=1234
        --do_train
        --ignore_idx=0
        --max_seq=128
        --use_prompt
        --prompt_len=4
        --sample_ratio=1.0
        --save_path=your_ckpt_path

--dataset_name=twitter17
      --bert_name=/home/nlp/code/HVPNeT/bert_base_uncased
      --seed=1234
      --only_test
      --max_seq=80
      --use_prompt
      --prompt_len=4
      --sample_ratio=1.0
      --load_path=/home/nlp/code/HVPNeT/your_ckpt_path/best_model.pth
--dataset_name=twitter15
        --bert_name=/home/nlp/code/HVPNeT/bert_base_uncased
        --num_epochs=30
        --batch_size=8
        --lr=3e-5
        --warmup_ratio=0.01
        --eval_begin_epoch=3
        --seed=1234
        --do_train
        --ignore_idx=0
        --max_seq=80
        --use_prompt
        --prompt_len=4
        --sample_ratio=1.0
        --save_path=your_ckpt_path
   --dataset_name=twitter17
        --bert_name=/home/nlp/code/HVPNeT/bert_base_uncased
        --num_epochs=50
        --batch_size=32
        --lr=3e-5
        --warmup_ratio=0.01
        --eval_begin_epoch=3
        --seed=1234
        --do_train
        --ignore_idx=0
        --max_seq=128
        --use_prompt
        --prompt_len=4
        --sample_ratio=1.0
        --save_path=your_ckpt_path
